{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple implementation of a Neural Network.\n",
    "\n",
    "    Attributes:\n",
    "        inodes (int): The number of input nodes.\n",
    "        hnodes (int): The number of hidden nodes.\n",
    "        onodes (int): The number of output nodes.\n",
    "        lr (float): The learning rate.\n",
    "        wih (numpy.ndarray): The weights between the input and hidden layers.\n",
    "        who (numpy.ndarray): The weights between the hidden and output layers.\n",
    "        activation_function (function): The activation function to use.\n",
    "\n",
    "    ðŸ¤– prompt: generate docs\n",
    "    \"\"\"\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # Weight sampling rule of thumb (page 103 book DLO): \n",
    "        #   - normal distribution: mean zero\n",
    "        #   - standard deviation: inverse of the square root of the number of links into a node\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,0.5), (self.hnodes, self.inodes))  # weights from input to hidden layer\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,0.5), (self.onodes, self.hnodes))  # weights from hidden to output layer\n",
    "        \n",
    "        self.lr = learningrate\n",
    "        self.activation_function = lambda x: special.expit(x)   # sigmoid function\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Trains the neural network on a single batch of inputs and targets.\n",
    "\n",
    "        Args:\n",
    "            inputs_list (list): A list of input values for a single batch of training examples.\n",
    "            targets_list (list): A list of target values for a single batch of training examples.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        ðŸ¤– prompt: generate docs\n",
    "        \"\"\"\n",
    "        # inputs_list = [1, 2, 3]\n",
    "        # inputs_array = [[1 2 3]]\n",
    "        # inputs_transposed = [[1]\n",
    "        #                      [2]\n",
    "        #                      [3]]\n",
    "        inputs = np.array(inputs_list, ndmin=2)                     # convert to 2d for matrix multiplication\n",
    "        inputs_transposed = inputs.T                                # transpose to ensure same orientation as weights (column vector)\n",
    "        targets = np.array(targets_list, ndmin=2)                   # convert to 2d for matrix multiplication\n",
    "        targets_transposed = targets.T                              # transpose to ensure same orientation as weights (column vector)\n",
    "        \n",
    "        hidden_inputs = self.wih @ inputs_transposed                # calculate signals into hidden layer, using matrix multiplication\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)    # pass through activation function to calculate final values\n",
    "        \n",
    "        final_inputs = self.who @ hidden_outputs                    # calculate signals into output layer, using matrix multiplication\n",
    "        final_outputs = self.activation_function(final_inputs)      # pass through activation function to calculate final values\n",
    "        \n",
    "        output_errors = targets_transposed - final_outputs          # output layer is the final layer, so error is (target - actual)\n",
    "        hidden_errors = self.who.T @ output_errors                  # errors propagated to the hidden layer\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * (output_errors * final_outputs * (1.0 - final_outputs)) @ hidden_outputs.T\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * (hidden_errors * hidden_outputs * (1.0 - hidden_outputs)) @ inputs\n",
    "        pass\n",
    "    \n",
    "    def query(self, inputs_list):\n",
    "        \"\"\"\n",
    "        Given an input list, returns the output of the neural network.\n",
    "\n",
    "        Args:\n",
    "            inputs_list (list): A list of input values for the neural network.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A 2D numpy array containing the output values of the neural network.\n",
    "\n",
    "        ðŸ¤– prompt: generate docs\n",
    "        \"\"\"\n",
    "        # inputs_list = [1, 2, 3]\n",
    "        # inputs_array = [[1 2 3]]\n",
    "        # inputs_transposed = [[1]\n",
    "        #                      [2]\n",
    "        #                      [3]]\n",
    "        inputs_array = np.array(inputs_list, ndmin=2)               # convert to 2d for matrix multiplication\n",
    "        inputs_transposed = inputs_array.T                          # transpose to ensure same orientation as weights (column vector)\n",
    "        \n",
    "        hidden_inputs = self.wih @ inputs_transposed                # calculate signals into hidden layer, using matrix multiplication\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)    # pass through activation function to calculate final values\n",
    "        \n",
    "        final_inputs = self.who @ hidden_outputs                    # calculate signals into output layer, using matrix multiplication\n",
    "        final_outputs = self.activation_function(final_inputs)      # pass through activation function to calculate final values\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33728861],\n",
       "       [0.50326998],\n",
       "       [0.67109129]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nodes = 3\n",
    "hidden_nodes = 1\n",
    "output_nodes = 3\n",
    "learning_rate = 0.1\n",
    "\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# In supervised learning, we would normally use many input-target pairs\n",
    "# and use each pair to update the network weights.\n",
    "inputs = [1.0, 0.5, -1.5]\n",
    "targets = [0.5, 0.5, 0.5]\n",
    "nn.train(inputs, targets)\n",
    "\n",
    "outputs = nn.query(inputs)\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
