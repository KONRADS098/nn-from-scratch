{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple implementation of a Neural Network.\n",
    "\n",
    "    Attributes:\n",
    "        inodes (int): The number of input nodes.\n",
    "        hnodes (int): The number of hidden nodes.\n",
    "        onodes (int): The number of output nodes.\n",
    "        wih (numpy.ndarray): The weights from the input layer to the hidden layer.\n",
    "        who (numpy.ndarray): The weights from the hidden layer to the output layer.\n",
    "        lr (float): The learning rate for the neural network.\n",
    "        activation_function (function): The activation function for the neural network.\n",
    "\n",
    "    🤖 prompt: generate docs\n",
    "    \"\"\"\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # Weight sampling rule of thumb (page 103 book DLO): \n",
    "        #   - normal distribution: mean zero\n",
    "        #   - standard deviation: inverse of the square root of the number of links into a node\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,0.5), (self.hnodes, self.inodes))  # weights from input to hidden layer\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,0.5), (self.onodes, self.hnodes))  # weights from hidden to output layer\n",
    "        \n",
    "        self.lr = learningrate\n",
    "        self.activation_function = lambda x: special.expit(x)   # sigmoid function\n",
    "\n",
    "    def forward(self, inputs_list):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            inputs_list (list): A list of input values for a single batch of training examples.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the inputs, hidden outputs, and final outputs as numpy arrays.\n",
    "\n",
    "        🤖 prompt: generate docs\n",
    "        \"\"\"\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        hidden_inputs = self.wih @ inputs                           # calculate signals into hidden layer, using matrix multiplication\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)    # pass through activation function to calculate final values\n",
    "        final_inputs = self.who @ hidden_outputs                    # calculate signals into output layer, using matrix multiplication\n",
    "        final_outputs = self.activation_function(final_inputs)      # pass through activation function to calculate final values\n",
    "        return inputs, hidden_outputs, final_outputs\n",
    "\n",
    "    def backward(self, targets_list, final_outputs, hidden_outputs, inputs):\n",
    "        \"\"\"\n",
    "        Performs the backward pass of the neural network, updating the weights based on the error of the output.\n",
    "\n",
    "        Args:\n",
    "            targets_list (list): A list of target values for a single batch of training examples.\n",
    "            final_outputs (numpy.ndarray): The final output values from the forward pass of the neural network.\n",
    "            hidden_outputs (numpy.ndarray): The hidden layer output values from the forward pass of the neural network.\n",
    "            inputs (numpy.ndarray): The input values for a single batch of training examples.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        🤖 prompt: generate docs\n",
    "        \"\"\"\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        output_errors = targets - final_outputs     # output layer is the final layer, so error is (target - actual)\n",
    "        hidden_errors = self.who.T @ output_errors  # errors propagated to the hidden layer\n",
    "\n",
    "        # calculate the gradient of the activation function (sigmoid)\n",
    "        self.who += self.lr * (output_errors * final_outputs * (1.0 - final_outputs)) @ hidden_outputs.T    # update the weights for the links between the hidden and output layers\n",
    "        self.wih += self.lr * (hidden_errors * hidden_outputs * (1.0 - hidden_outputs)) @ inputs.T          # update the weights for the links between the input and hidden layers\n",
    "        pass\n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Trains the neural network on a single batch of inputs and targets.\n",
    "\n",
    "        Args:\n",
    "            inputs_list (list): A list of input values for a single batch of training examples.\n",
    "            targets_list (list): A list of target values for a single batch of training examples.\n",
    "\n",
    "        Returns:    \n",
    "            None\n",
    "\n",
    "        🤖 prompt: generate docs\n",
    "        \"\"\"\n",
    "        inputs, hidden_outputs, final_outputs = self.forward(inputs_list)   # forward pass\n",
    "        self.backward(targets_list, final_outputs, hidden_outputs, inputs)  # backward pass\n",
    "\n",
    "    def query(self, inputs_list):\n",
    "        \"\"\"\n",
    "        Given an input list, returns the output of the neural network.\n",
    "\n",
    "        Args:\n",
    "            inputs_list (list): A list of input values for the neural network.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A 2D numpy array containing the output values of the neural network.\n",
    "\n",
    "        🤖 prompt: generate docs\n",
    "        \"\"\"\n",
    "        _, _, final_outputs = self.forward(inputs_list)     # forward pass\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48942926],\n",
       "       [0.48160294],\n",
       "       [0.59299874]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nodes = 3\n",
    "hidden_nodes = 1\n",
    "output_nodes = 3\n",
    "learning_rate = 0.1\n",
    "\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# In supervised learning, we would normally use many input-target pairs\n",
    "# and use each pair to update the network weights.\n",
    "inputs = [1.0, 0.5, -1.5]\n",
    "targets = [0.5, 0.5, 0.5]\n",
    "nn.train(inputs, targets)\n",
    "\n",
    "outputs = nn.query(inputs)\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
